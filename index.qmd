---
format: 
  revealjs:
    footer: "Packages for signal processing and forecasting"
    logo: "gfx/delphi.jpg"
    embed-resources: false
    chalkboard: true
    width: 1280
    height: 720
    theme: [default, themer.scss]
    fig-format: svg
    html-math-method: mathjax
execute: 
  cache: true
editor: source
---

```{r}
#| fig-align: center
#| fig-format: svg
primary <- "#a8201a"
secondary <- "#f9c80e"
tertiary <- "#2a76dd"
fourth_colour <- "#311847"
library(epiprocess)
library(epipredict)
suppressMessages(library(tidyverse))
x <- archive_cases_dv_subset
x_latest <- epix_as_of(x, max_version = max(x$DT$version))
self_max = max(x$DT$version)
versions = seq(as.Date("2020-06-01"), self_max - 1, by = "1 month")
snapshots_all <- map_dfr(versions, function(v) { 
  epix_as_of(x, max_version = v) %>% mutate(version = v)}) %>%
  bind_rows(x_latest %>% mutate(version = self_max)) %>%
  mutate(latest = version == self_max)
snapshots <- snapshots_all %>% 
  filter(geo_value %in% c("ca", "fl"))
theme_set(theme_bw(base_size = 16))
```

```{r}
#| include: false
#| label: cover-art
ggplot(snapshots_all %>% 
         arrange(geo_value, version, time_value) %>% 
         filter(!latest),
       aes(x = time_value, y = percent_cli)) +  
  geom_line(aes(color = factor(version), group = interaction(geo_value, version))) + 
  #geom_vline(aes(color = factor(version), xintercept = version), lty = 3, 
  #           size = 0.5) +
  scale_x_date(minor_breaks = "month", labels = NULL) +
  labs(x = "", y = "") + 
  theme_void() +
  coord_cartesian(xlim = as.Date(c("2020-10-01", NA)), ylim = c(-5, NA)) +
  scale_color_viridis_d(option = "B", end = .8) +
  theme(legend.position = "none", panel.background = element_blank()) +
  geom_line(
    data = snapshots %>% filter(latest),
    aes(x = time_value, y = percent_cli, group = geo_value), 
    inherit.aes = FALSE, color = "black")
```



:::: {.columns}
::: {.column width="20%"}

:::
::: {.column width="80%"}
##  `{epiprocess}` and `{epipredict}` {background-image="index_files/figure-revealjs/cover-art-1.svg" background-position="bottom"}

### `R` packages for signal processing and forecasting

<br>

#### Daniel J. McDonald, Ryan J. Tibshirani, and Logan C. Brooks
#### and CMU's Delphi Group

CDC Flu Site Visit --- 19 July 2023
:::
::::


## Background

::: {.incremental}

* Covid-19 Pandemic required quickly implementing forecasting systems.
* Basic processing---[outlier detection]{.primary}, [reporting issues]{.secondary}, [geographic granularity]{.tertiary}---implemented in parallel / error prone
* Data revisions complicate evaluation
* Simple models often outperformed complicated ones
* Custom software not easily adapted / improved by other groups
* Hard for public health actors to borrow / customize community techniques

:::


## Packages under development

```{mermaid}
%%| fig-width: 10
%%| fig-height: 7
flowchart LR
  A("{epidatr}") --> C("{epiprocess}")
  B("{epidatpy}") --> C
  D("{covidcast}") --> C
  C --> E("{epipredict}")
```

# `{epiprocess}`
[Tools for signal processing]{.primary}

## Basic processing operations and data structures

::: {.incremental}

* General EDA for panel data
* Calculate rolling statistics
* Fill / impute gaps
* Examine correlations, growth rates
* Store revision history smartly
* Inspect revision patterns
* Find / correct outliers

:::

## `epi_df`: snapshot of a data set

* a tibble; requires columns `geo_value` and `time_value`.
* arbitrary additional columns containing [measured values]{.primary}
* additional [keys]{.primary} to index (`age_group`, `ethnicity`, etc.)

::: {.callout-note}
## `epi_df`

Represents a [snapshot]{.primary} that
contains the most [up-to-date values]{.primary} of the signal variables, [as of]{.primary} a given time.
:::

## `epi_df`: snapshot of a data set

```{css}
.withscroll {
    height: 55vh;
    overflow-y: auto !important;
}
```

```{r}
#| output: asis

withr::with_options(
  code={
    cat("<details><summary>Example data object documentation, license, attribution</summary>")
    cat('<div class="withscroll">')
    print(help("case_death_rate_subset", package="epipredict", help_type="text"))
    cat("</div>")
    cat("</details>")
  },
  list(pager=function(files, header, title, delete.file) {
    on.exit({
      unlink(files)
    })
    cat(paste(c("<pre>",purrr::reduce(purrr::map(files, function(file) {
      # gsub("</?u>","_",gsub("</u>( *)<u>","\\1",
      gsub("_\b(.)", "<u>\\1</u>", readLines(file))
      # ))
    }), function(x, y) c(x,"\n\n\n",y)), "</pre>"), collapse="\n"))
  })
)
```

```{r}
edf <- case_death_rate_subset
edf
```

## Sliding examples on `epi_df`

### Correlations at different lags

```{r, echo=TRUE}
cor0 <- epi_cor(edf, case_rate, death_rate, cor_by = time_value)
cor14 <- epi_cor(edf, case_rate, death_rate, cor_by = time_value, dt1 = -14)
```

```{r}
#| fig-align: center
#| fig-format: svg
rbind(
  cor0 %>% mutate(lag = 0),
  cor14 %>% mutate(lag = 14)
) %>%
  mutate(lag = as.factor(lag)) %>%
  ggplot(aes(x = time_value, y = cor)) +
  geom_hline(yintercept = 0, size = 1.5) +
  geom_line(aes(color = lag), size = 1.5) +
  scale_color_brewer(palette = "Set1") +
  scale_x_date(minor_breaks = "month", date_labels = "%b %Y") +
  labs(x = "Date", y = "Correlation", col = "Lag")
```

## Outlier detection and correction

```{r}
ny <- jhu_csse_daily_subset %>%
  filter(geo_value == "ny") %>%
  select(time_value, geo_value, cases)
```

```{r}
#| echo: true
ny <- ny %>% mutate(outliers = detect_outlr_stl(time_value, cases))
```

```{r}
#| fig-align: center
#| fig-format: svg
ggplot(ny, aes(x = time_value)) + 
  geom_ribbon(
    aes(ymin = outliers$lower / 1000, ymax = outliers$upper / 1000), 
    fill = "lightgrey") +
  geom_line(aes(y = cases / 1000), color = primary) + 
  geom_point(
    data = function(df) df %>% filter(cases != outliers$replacement),
    aes(y = outliers$replacement / 1000), color = tertiary, size = 3) +
  scale_y_continuous(
    limits = c(0, NA), 
    expand = expansion(mult = c(0, 0.05))) +
  scale_x_date(date_labels = "%b %Y") +
  labs(x = "", y = "incident cases\n in New York (1000's)")
```




## Sliding examples on `epi_df`

### Growth rates

```{r, echo=TRUE}
edf <- filter(edf, geo_value %in% c("ut", "ca")) %>%
  group_by(geo_value) %>%
  mutate(gr_cases = growth_rate(time_value, case_rate, method = "trend_filter"))
```

```{r}
#| fig-align: center
#| fig-format: svg
ggplot(edf, aes(x = time_value, y = gr_cases)) +
  geom_hline(yintercept = 0, size = 1.5) +
  geom_line(aes(col = geo_value), size = 1.5) +
  geom_hline(yintercept = 0) +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_color_manual(values = c(3, 6)) +
  scale_x_date(minor_breaks = "month", date_labels = "%b %Y") +
  labs(x = "Date", y = "Growth rate", col = "State")
```

## `epi_archive`: collection of `epi_df`s

* full version history of a data set
* acts like a bunch of `epi_df`s --- but stored [compactly]{.primary}
* allows similar funtionality as `epi_df` but using only [data that would have been available at the time]{.primary}

::: {.callout-note}
## Revisions

Epidemiology data gets revised frequently. (Happens in Economics as well.) 

* We may want to use the data [as it looked in the past]{.primary} 
* or we may want to examine [the history of revisions]{.primary}.
:::


## Revision patterns

```{r}
#| output: asis

withr::with_options(
  code={
    cat("<details><summary>Example data object documentation, license, attribution</summary>")
    cat('<div class="withscroll">')
    print(help("archive_cases_dv_subset", package="epiprocess", help_type="text"))
    cat("</div>")
    cat("</details>")
  },
  list(pager=function(files, header, title, delete.file) {
    on.exit({
      unlink(files)
    })
    cat(paste(c("<pre>",purrr::reduce(purrr::map(files, function(file) {
      # gsub("</?u>","_",gsub("</u>( *)<u>","\\1",
      gsub("_\b(.)", "<u>\\1</u>", readLines(file))
      # ))
    }), function(x, y) c(x,"\n\n\n",y)), "</pre>"), collapse="\n"))
  })
)
```

```{r}
#| fig-align: center
#| fig-format: svg
ggplot(snapshots %>% filter(!latest),
            aes(x = time_value, y = percent_cli)) +  
  geom_line(aes(color = factor(version))) + 
  geom_vline(aes(color = factor(version), xintercept = version), lty = 3) +
  facet_wrap(~ geo_value, scales = "free_y", nrow = 1) +
  scale_x_date(minor_breaks = "month", date_labels = "%b %Y") +
  labs(x = "", y = "% of doctor's visits with\n Covid-like illness") + 
  scale_color_viridis_d(option = "B", end = .8) +
  theme(legend.position = "none") +
  geom_line(data = snapshots %>% filter(latest),
               aes(x = time_value, y = percent_cli), 
               inherit.aes = FALSE, color = "black")
```

## Latency investigation

How stale is my data over space and time?

There are (lots) of possible definitions.

[Nominal latency]{.primary}
: the difference between the max `time_value` available for some location, as of some version

[Nonmissing latency]{.primary}
: nominal + recorded `NA`s and skipped `time_value`s are latent

[Nonmissing nonzero latency]{.secondary}
: nonmissing + zeros are latent

[Nonmissing nonduplicate latency]{.primary}
: nonmissing + duplications of the most recent non-`NA` values are latent


## Latency investigation

```{r}
#| echo: false
#| eval: false
hhs_flu_hosp_archive <- readRDS(here::here("data", "hhs_flu_hosp_dt.rds")) %>%
  as_epi_archive(compactify = TRUE)
```

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "|5|6|8-9|14"
pos_first_true <- function(x) match(TRUE, rev(x), length(x) + 1) - 1

na0_latency <- function(dat, group_key, ref_time_value) {
  group_by(dat, geo_value) %>%
    complete(time_value = full_seq(time_value, period = 1L)) %>%
    mutate(not_na0 = !(is.na(admissions) | admissions == 0L)) %>%
    summarize(
      nominal_latency = as.integer(ref_time_value - max(time_value)),
      na0_latency = pos_first_true(not_na0) + nominal_latency
    )
}

hhs_flu_hosp_archive %>% 
  epix_slide(na0_latency, before = 27, names_sep = NULL) %>% 
  rename(version = time_value)
```


## Latency investigation, > 2 days

```{r}
#| fig-format: jpeg
#| fig-align: center
#| dev: jpeg
hhs_na0_latency <- readRDS(here::here("data", "hhs_na0_latency.rds"))
hhs_na0_latency %>%
  mutate(na0_latency = case_when(
    na0_latency <= 2 ~ NA,
    TRUE ~ na0_latency
  )) %>%
  ggplot(aes(version, geo_value, fill = na0_latency)) +
  geom_raster() +
  scale_fill_viridis_c(
    na.value = "transparent", name = "NMNZ latency",
    guide = guide_colorbar(barheight = 12)) +
  theme_bw(16) +
  scale_x_date(date_labels = "%b %Y", expand = expansion()) +
  labs(xlab = "Version", ylab = "") 
```

# `{epipredict}`
[A forecasting framework]{.primary}

## Philosophy of forecasting 

::: {.fragment .fade-in-then-semi-out}

We should build up modular components

Be able to add layers sequentially

:::

::: {.fragment .fade-in-then-semi-out}

1. [Preprocessor:]{.primary} do things to the data before model training
2. [Trainer:]{.primary} train a model on data, resulting in an object
3. [Predictor:]{.primary} make predictions, using a fitted model object
4. [Postprocessor:]{.primary} do things to the predictions before returning

:::


::: {.fragment .fade-in}
A very specialized plug-in to [`{tidymodels}`](https://tidymodels.org)
:::


## `{epipredict}` 

::: {.incremental}

* Canned forecasters that work out-of-the-box
* Backtest using the versioned data
* Easily create features
* Modify / transform / calibrate forecasts 
* Quickly pivot to new tasks
* Highly customizable for advanced users 

:::

## Canned forecasters that work out of the box.

But, you can adjust [a lot]{.secondary} of options

### We currently provide:

- Baseline flat-line forecaster
- Autoregressive-type forecaster
- Autoregressive-type classifier
- "Smooth" autoregressive-type forecaster

## Canned forecasters that work out of the box.

But, you can adjust [a lot]{.secondary} of options

### Example forecasting task


* On day $t$

* We want to predict 
  - new hospitalizations $y$, 
  - $h$ days ahead, 
  - at many locations $j$.

* We're going to make a new forecast each week.



## Canned forecasters that work out of the box.

But, you can adjust [a lot]{.secondary} of options

### Flatline forecaster

For each location, predict 
$$\hat{y}_{j,\ t+h} = y_{j,\ t}$$

## Canned forecasters that work out of the box.

But, you can adjust [a lot]{.secondary} of options

### AR forecaster

Use an AR model with an extra feature, e.g.:
$$\hat{y}_{j,\ t+h} = \mu + a_0 y_{j,\ t} + a_7 y_{j,\ t-7} + b_0 x_{j,\ t} + b_7 x_{j,\ t-7}$$



## Basic autoregressive forecaster

* Predict `death_rate`, 1 week ahead, with `0,7,14` day lags of `cases` and `deaths`. 
* Use `lm` for estimation. Also create intervals.

```{r}
#| echo: true
#| warning: false
edf <- case_death_rate_subset # grab some built-in data
canned <- arx_forecaster(
  epi_data = edf, 
  outcome = "death_rate", 
  predictors = c("case_rate", "death_rate")
)
```

The output is a model object that could be reused in the future, along with the predictions for 7 days from now.

## Adjust lots of built-in options

```{r}
#| echo: true
#| eval: false
#| code-line-numbers: "|4|5|7|8-12|13|14"
rf <- arx_forecaster(
  epi_data = edf, 
  outcome = "death_rate", 
  predictors = c("case_rate", "death_rate", "fb-survey"),
  trainer = parsnip::rand_forest(mode = "regression"), # use {ranger}
  args_list = arx_args_list(
    ahead = 14, # 2-week horizon
    lags = list(
      case_rate = c(0:4, 7, 14), 
      death_rate = c(0, 7, 14), 
      `fb-survey` = c(0:7, 14)
    ),
    levels = c(0.01, 0.025, 1:19/20, 0.975, 0.99), # 23 ForecastHub quantiles
    quantile_by_key = "geo_value" # vary noise model by location
  )
)
```



## Do (almost) anything manually

```{r}
#| echo: true
#| eval: false
#| code-line-numbers: "1-6|8-13|15-17|19-26"
# A preprocessing "recipe" that turns raw data into features / response
r <- epi_recipe(edf) %>%
  step_epi_lag(case_rate, lag = c(0, 1, 2, 3, 7, 14)) %>%
  step_epi_lag(death_rate, lag = c(0, 7, 14)) %>%
  step_epi_ahead(death_rate, ahead = 14) %>%
  step_epi_naomit()

# A postprocessing routine describing what to do to the predictions
f <- frosting() %>%
  layer_predict() %>%
  layer_threshold(.pred, lower = 0) %>% # predictions / intervals should be non-negative
  layer_add_target_date(target_date = max(edf$time_value) + 14) %>%
  layer_add_forecast_date(forecast_date = max(edf$time_value))

# Bundle up the preprocessor, training engine, and postprocessor
# We use quantile regression
ewf <- epi_workflow(r, quantile_reg(tau = c(.1, .5, .9)), f)

# Fit it to data (we could fit this to ANY data that has the same format)
trained_ewf <- ewf %>% fit(edf)

# examines the recipe to determine what we need to make the prediction
latest <- get_test_data(r, edf)

# we could make predictions using the same model on ANY test data
preds <- trained_ewf %>% predict(new_data = latest)
```

## Visualize a result for 1 forecast date, 1 location

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
#| fig-format: svg
fd <- as.Date("2021-11-30")
geos <- c("ut", "ca")

tedf <- edf %>% filter(time_value >= fd)
# use most recent 3 months for training
edf <- edf %>% filter(time_value < fd, time_value >= fd - 90L)

rec <- epi_recipe(edf) %>%
  step_epi_lag(case_rate, lag = c(0, 7, 14, 21)) %>%
  step_epi_lag(death_rate, lag = c(0, 7, 14)) %>%
  step_epi_ahead(death_rate, ahead = 1:28)

f <- frosting() %>%
  layer_predict() %>%
  layer_unnest(.pred) %>%
  layer_naomit(distn) %>%
  layer_add_forecast_date() %>%
  layer_threshold(distn)

ee <- smooth_quantile_reg(
  tau = c(.1, .25, .5, .75, .9),
  outcome_locations = 1:28,
  degree = 3L
)

ewf <- epi_workflow(rec, ee, f)
  
the_fit <- ewf %>% fit(edf)

latest <- get_test_data(rec, edf, fill_locf = TRUE)
preds <- predict(the_fit, new_data = latest) %>%
  mutate(forecast_date = fd, target_date = fd + ahead) %>%
  select(geo_value, target_date, distn) %>%
  pivot_quantiles(distn) %>%
  filter(geo_value %in% geos)

ggplot(preds) +
  geom_ribbon(aes(target_date, ymin = `0.1`, ymax = `0.9`),
              fill = "cornflowerblue", alpha = .8) +
  geom_ribbon(aes(target_date, ymin = `0.25`, ymax = `0.75`),
              fill = "#00488E", alpha = .8) +
  geom_line(data = bind_rows(tedf, edf) %>% filter(geo_value %in% geos), 
            aes(time_value, death_rate), size = 1.5) +
  geom_line(aes(target_date, `0.5`), color = "orange", size = 1.5) +
  geom_vline(xintercept = fd) +
  facet_wrap(~geo_value) +
  theme_bw(16) +
  scale_x_date(name = "", date_labels = "%b %Y") +
  ylab("Deaths per 100K inhabitants")
```



# Questions?

